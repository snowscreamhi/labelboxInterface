{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Labelbox - Spring 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ty8bb0dM5MKU",
        "ImaW1yll5FN5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt3194VF2zPw"
      },
      "source": [
        "# Labelbox - Spring 2021\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq0d8jpaE8xf"
      },
      "source": [
        "This notebook details different concepts for Labelbox integration, and was used for the Qualitative Analysis project for the IBM Watson in the Watt CI. The purpose of this semester's work was to test triplet-based learning through the Labelbox platform with a custom labeling interface. This allows researchers to quickly label triplets of data, and this notebook contains information on how to set up the Labelbox data. The main sections of this notebook are:\n",
        "\n",
        "\n",
        "*   Generating Labelbox-compatible triplet data from a text dataset\n",
        "*   Training a tokenizer\n",
        "*   Creating an active learning pipeline for Labelbox\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMOipgksDpgc"
      },
      "source": [
        "## General Notes:\n",
        "\n",
        "- For the purposes of this document, I changed most filenames to \\<filename\\>_example to ensure that none of the original documents got messed up. If you intend to pick up and use this code, it would be best practice to rename these files to something more descriptive.\n",
        "- json.loads() seems to frequently throw \"invalid escape character\" errors with this data, likely due to the way it was formatted originally. This can often be easily fixed with a .replace() function to get rid of the invalid character. However, I recommend using a more robust data cleaning method that can ensure no invalid characters are present.\n",
        "- Similar escape character issues are present during GraphQL queries, which can also be resolved using .replace(). Again, it would be best practice to use a more robust data cleaning method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h83j5o45NJYG"
      },
      "source": [
        "-Quinn Hubbarth (quinn.hubbarth@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty8bb0dM5MKU"
      },
      "source": [
        "# Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwjBrid55Lfa",
        "outputId": "3907e9ed-7b3d-4f05-a1f5-40511f03bca2"
      },
      "source": [
        "!pip install tokenizers\n",
        "!pip install labelbox\n",
        "from labelbox import Client, Dataset, LabelingFrontend, Project, DataRow\n",
        "from labelbox import schema\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import json\n",
        "from itertools import chain, islice\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from tokenizers.normalizers import Sequence, Lowercase, Strip\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: labelbox in /usr/local/lib/python3.7/dist-packages (2.5.4)\n",
            "Requirement already satisfied: google-api-core>=1.22.1 in /usr/local/lib/python3.7/dist-packages (from labelbox) (1.26.3)\n",
            "Requirement already satisfied: backoff==1.10.0 in /usr/local/lib/python3.7/dist-packages (from labelbox) (1.10.0)\n",
            "Requirement already satisfied: pydantic==1.8 in /usr/local/lib/python3.7/dist-packages (from labelbox) (1.8)\n",
            "Requirement already satisfied: ndjson==0.3.1 in /usr/local/lib/python3.7/dist-packages (from labelbox) (0.3.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from labelbox) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (1.28.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (56.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core>=1.22.1->labelbox) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic==1.8->labelbox) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->labelbox) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->labelbox) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->labelbox) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->labelbox) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core>=1.22.1->labelbox) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core>=1.22.1->labelbox) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1xtXoBi5W04",
        "outputId": "6478038d-9d02-4339-d76c-dad76408e6cf"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImaW1yll5FN5"
      },
      "source": [
        "# Generating Labelbox-compatible triplet data from a text dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "jcr1Zmik2XRz",
        "outputId": "3e69530e-1ec2-44fc-e7c5-ccd6657242c7"
      },
      "source": [
        "# Define a datapath based on the structure of your Google Drive. One datapath might look like this:\n",
        "dataPath = 'gdrive/My Drive/Semester 6/UPIC_WitW/data/'\n",
        "\n",
        "# Read in a dataset of your choosing. In this instance, we used Amazon Fine Food Reviews, which could look like this:\n",
        "database = pd.read_csv(dataPath + \"Reviews.csv\")\n",
        "database.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "wCZDQ6NM6DFd",
        "outputId": "f8e7d0f2-b97f-4541-bb94-c6703063e269"
      },
      "source": [
        "# To prepare the data for separating into triplets, we should select the Summary and Text columns from the database, combine them, and make them readable for JSON & HTML formatting:\n",
        "reviews = pd.DataFrame(\"<b>\" + database[\"Summary\"].str.replace('\"', '').str.replace(\"{\", \"(\").str.replace(\"}\", \")\") + \":<br \\/> </b>\" + database[\"Text\"].str.replace('\"', '').str.replace(\"{\", \"(\").str.replace(\"}\", \")\"))\n",
        "\n",
        "# To separate the data into triplets, we must generate a set of random indices from the data. This creates variable \"indices\", which creates random arrays of 3 indexes.\n",
        "indices = np.random.randint(0, reviews.size, ((int)(reviews.size/3), 3))\n",
        "\n",
        "# These indices are then nicely sorted into a Pandas dataframe.\n",
        "reviews_indices = pd.DataFrame(indices, columns = {\"A_id\", \"B_id\", \"C_id\"})\n",
        "reviews_indices = reviews_indices.reindex([\"A_id\", \"B_id\", \"C_id\"], axis = 1)\n",
        "reviews_indices.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A_id</th>\n",
              "      <th>B_id</th>\n",
              "      <th>C_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>479638</td>\n",
              "      <td>561281</td>\n",
              "      <td>467604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>331429</td>\n",
              "      <td>96362</td>\n",
              "      <td>146627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>322464</td>\n",
              "      <td>193204</td>\n",
              "      <td>52168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>98227</td>\n",
              "      <td>110668</td>\n",
              "      <td>374978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>484180</td>\n",
              "      <td>305031</td>\n",
              "      <td>494406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     A_id    B_id    C_id\n",
              "0  479638  561281  467604\n",
              "1  331429   96362  146627\n",
              "2  322464  193204   52168\n",
              "3   98227  110668  374978\n",
              "4  484180  305031  494406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "qKfzm8dt75ph",
        "outputId": "714ca19b-48ee-4c0f-cfd4-dcb8e543c721"
      },
      "source": [
        "# Creating reviews_triplets uses the indices to pull data from the reviews dataframe, and concatenates the triplets of reviews onto the triplets of indices.\n",
        "\n",
        "reviews_triplets = pd.concat([pd.DataFrame(reviews_indices.apply(lambda x: x.apply(lambda y: reviews[0][y]))).rename(columns = {\"A_id\":\"A\", \"B_id\": \"B\", \"C_id\": \"C\"}), reviews_indices], axis=1)\n",
        "reviews_triplets.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>A_id</th>\n",
              "      <th>B_id</th>\n",
              "      <th>C_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;b&gt;coffee that will wake you up!:&lt;br \\/&gt; &lt;/b&gt;I...</td>\n",
              "      <td>&lt;b&gt;Hemp it Up!!!:&lt;br \\/&gt; &lt;/b&gt;I love this stuff...</td>\n",
              "      <td>&lt;b&gt;Biggest disappointment ever:&lt;br \\/&gt; &lt;/b&gt;I d...</td>\n",
              "      <td>479638</td>\n",
              "      <td>561281</td>\n",
              "      <td>467604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;b&gt;Not bad, but the original Gold Bears are be...</td>\n",
              "      <td>&lt;b&gt;A real treat!!:&lt;br \\/&gt; &lt;/b&gt;This jam is abso...</td>\n",
              "      <td>&lt;b&gt;Melitta Coffee Pods:&lt;br \\/&gt; &lt;/b&gt;tastes ok b...</td>\n",
              "      <td>331429</td>\n",
              "      <td>96362</td>\n",
              "      <td>146627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;b&gt;not good:&lt;br \\/&gt; &lt;/b&gt;I ordered this because...</td>\n",
              "      <td>&lt;b&gt;Really great - price outrageous:&lt;br \\/&gt; &lt;/b...</td>\n",
              "      <td>&lt;b&gt;Yummy Delicious Sunbutter:&lt;br \\/&gt; &lt;/b&gt;Thing...</td>\n",
              "      <td>322464</td>\n",
              "      <td>193204</td>\n",
              "      <td>52168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;b&gt;Very convenient:&lt;br \\/&gt; &lt;/b&gt;I have a confes...</td>\n",
              "      <td>&lt;b&gt;Why is the decaf almost twice as expensive?...</td>\n",
              "      <td>&lt;b&gt;Teeny, tiny amount:&lt;br \\/&gt; &lt;/b&gt;I haven't ac...</td>\n",
              "      <td>98227</td>\n",
              "      <td>110668</td>\n",
              "      <td>374978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;b&gt;coffee:&lt;br \\/&gt; &lt;/b&gt;I love this coffee, and ...</td>\n",
              "      <td>&lt;b&gt;Love Crunch Granola:&lt;br \\/&gt; &lt;/b&gt;If you like...</td>\n",
              "      <td>&lt;b&gt;Perhaps I don't love green tea as much as t...</td>\n",
              "      <td>484180</td>\n",
              "      <td>305031</td>\n",
              "      <td>494406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   A  ...    C_id\n",
              "0  <b>coffee that will wake you up!:<br \\/> </b>I...  ...  467604\n",
              "1  <b>Not bad, but the original Gold Bears are be...  ...  146627\n",
              "2  <b>not good:<br \\/> </b>I ordered this because...  ...   52168\n",
              "3  <b>Very convenient:<br \\/> </b>I have a confes...  ...  374978\n",
              "4  <b>coffee:<br \\/> </b>I love this coffee, and ...  ...  494406\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Y5JNqL-EKx",
        "outputId": "1a907712-51a0-4d7c-8646-13c98f8f13cd"
      },
      "source": [
        "# Now, we simply need to translate this data into the way Labelbox likes it. This involves replacing certain characters with valid characters for JSON strings.\n",
        "# We also will export the data by specifying the amount of triplets we want to use. You can export as many triplets as there are rows in reviews_triplets.\n",
        "# To export the data, we use the pandas function .to_json, which takes in a parameter \"orient\". The closest export orient to the Labelbox requirements is \"records,\" but some formatting replacements still need to be made.\n",
        "\n",
        "num_to_export = 1000\n",
        "output_file = \"reviews_triplets_example.json\"\n",
        "\n",
        "reviews_triplets.json = open(dataPath + output_file, \"w\")\n",
        "reviews_triplets.json.write(reviews_triplets.head(num_to_export).to_json(orient=\"records\").replace('\"', \"\\\\\" + '\"').replace(\"{\", '{\\\"data\\\": \\\"{\\\\\"compare\\\\\":{').replace(\"}\", '}}\\\"}').replace(\"},\", \"},\\n\"))\n",
        "reviews_triplets.json.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT-hypob_cyG"
      },
      "source": [
        "TODO FOR THIS SECTION:\n",
        "\n",
        "- Clean up some of the .replace() functions with a more sophisticated way to clean up the data, if possible\n",
        "- Add more parameters in the data gathering function so users can easily swap out datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQV-sK5l_xHW"
      },
      "source": [
        "# Training a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A74l1o8f_zJl"
      },
      "source": [
        "# Specify a dataPath and database, similarly to the data generation style\n",
        "dataPath = 'gdrive/My Drive/Semester 6/UPIC_WitW/data/'\n",
        "database = pd.read_csv(dataPath + \"Reviews.csv\")\n",
        "\n",
        "# Set other settings for the tokenizer and tokenizer output\n",
        "min_freq=3\n",
        "vocab_size = 20000\n",
        "\n",
        "save_path = dataPath + \"reviews_example.txt\"\n",
        "save_path_tokenizer = dataPath + \"tokenizer%i-minf%i_example.json\" % (vocab_size, min_freq)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WN14ji3AOLN"
      },
      "source": [
        "# When creating a tokenizer, we thought it would be useful to make sure that none of the existing triplets were in the test set.\n",
        "# So, we created a method of getting the indices from the exported files in the above tutorial.\n",
        "# Then, we make sure to exclude this data from the test set, and add it back into the training set.\n",
        "\n",
        "# Read in the original exported dataset.\n",
        "reviews_json = pd.read_json(dataPath + 'reviews_triplets_example.json')\n",
        "index_list = []\n",
        "size = reviews_json.size\n",
        "\n",
        "# 1 is added to the indices because the \"id\" column in the data starts at index 1, not index 0.\n",
        "for i in range(size):\n",
        "  # Add each index in a triplet to index_list, which will eventually have all indexes in reviews_json\n",
        "  index_list.append(json.loads(reviews_json['data'][i])['compare']['A_id'] + 1)\n",
        "  index_list.append(json.loads(reviews_json['data'][i])['compare']['B_id'] + 1)\n",
        "  index_list.append(json.loads(reviews_json['data'][i])['compare']['C_id'] + 1)\n",
        "\n",
        "# Export these ids in case they are useful later.\n",
        "pd.DataFrame(index_list).to_csv(dataPath + 'reviews_triplets_example_indices.csv')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-hvmMWdBFQH"
      },
      "source": [
        "# Remove triplets we found from the train-test split \n",
        "# We do a little pandas work here to get a subset of dataset where all items in data_no_triplets have an \"Id\" that does not exist in index_list.\n",
        "data_no_triplets = database[~database.isin(index_list)[\"Id\"]]\n",
        "train_set, test_set = train_test_split(data_no_triplets, train_size = .9)\n",
        "\n",
        "# Add removed triplet data (3000 documents) to train set\n",
        "train_set.append(database[database.isin(index_list)[\"Id\"]])\n",
        "\n",
        "# Save train and test data\n",
        "train_set.to_csv(dataPath + \"train_example.csv\")\n",
        "test_set.to_csv(dataPath + \"test_example.csv\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84QurT89DInz"
      },
      "source": [
        "# Reload train data from disk\n",
        "train_data = pd.read_csv(dataPath + \"train_example.csv\")\n",
        "\n",
        "# Load and prep the data\n",
        "data = pd.DataFrame(train_data[\"Summary\"] + \": \"  + train_data[\"Text\"])\n",
        "\n",
        "# This does something to the data to make it useful for the tokenizer. Not quite sure what exactly\n",
        "text = data.melt()['value'].dropna()\n",
        "\n",
        "# Save the text data to specified save path\n",
        "text.to_csv(save_path, index=None, header=False)\n",
        "\n",
        "# Create specific special tokens\n",
        "special_toks=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe80Hj_MDTY3"
      },
      "source": [
        "# Tokenizer Training function, and tokenizer training\n",
        "def create_tokenizer(vocab_size, min_freq, special_toks, save_path_text, save_path_tokenizer):\n",
        "    tokenizer = Tokenizer(BPE())\n",
        "    tokenizer.normalizer = Sequence([Strip()])\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    trainer = BpeTrainer(special_tokens=special_toks, vocab_size=vocab_size, min_frequency=min_freq)\n",
        "    \n",
        "    tokenizer.train([save_path_text], trainer)\n",
        "    \n",
        "    tokenizer.post_processor = TemplateProcessing(\n",
        "        single=\"<s> $A </s>\",\n",
        "        special_tokens=[\n",
        "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "            (\"</s>\", tokenizer.token_to_id(\"</s>\"))\n",
        "        ],\n",
        "    )\n",
        "    \n",
        "    tokenizer.save(save_path_tokenizer)\n",
        "    \n",
        "    return tokenizer\n",
        "\n",
        "#Create tokenizer\n",
        "tokenizer = create_tokenizer(vocab_size, min_freq, special_toks, save_path, save_path_tokenizer)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d617roFzFVIq",
        "outputId": "09a89dbc-5441-45ce-a4ed-8e6d80cf6923"
      },
      "source": [
        "# Testing the tokenizer\n",
        "sentence = \"hello, y'all! [noise] how are \\0xg you 😁 ?\"\n",
        "print(tokenizer.encode(sentence).tokens)\n",
        "print(tokenizer.encode(sentence).ids)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', 'hello', ',', 'y', \"'\", 'all', '!', '[', 'noise', ']', 'how', 'are', 'x', 'g', 'you', '?', '</s>']\n",
            "[1, 18119, 17, 94, 12, 242, 6, 64, 10636, 66, 629, 204, 93, 76, 197, 36, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n0qOebxFfC_"
      },
      "source": [
        "# Creating an active learning pipeline for Labelbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4G0XhPUIAja"
      },
      "source": [
        "The motivation for the Active Learning Pipeline was based largely on this tutorial:\n",
        "https://labelbox.com/blog/active-learning-with-uncertainty-sampling/\n",
        "\n",
        "Also, the Labelbox GraphQL documentation was very useful to figure out how to use some of these functions:\n",
        "https://docs.labelbox.com/graphql-api/en/index-en#labeling-parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVO8E1rPG_IX"
      },
      "source": [
        "# Define datapath and read in the exported triplets.\n",
        "dataPath = 'gdrive/My Drive/Semester 6/UPIC_WitW/data/'\n",
        "reviews_json = pd.read_json(dataPath + 'reviews_triplets_example.json')\n",
        "\n",
        "# Register Labelbox client, project, and dataset using the Labelbox API.\n",
        "LABELBOX_API_KEY = \"get your own API Key\"\n",
        "project_name = \"Qualitative Analysis Triplets\"\n",
        "\n",
        "client = Client(LABELBOX_API_KEY)\n",
        "projects = client.get_projects(where=Project.name == project_name)\n",
        "project = next(iter(projects))\n",
        "\n",
        "# Collecting the dataset is interesting, because if you have multiple datasets of the same name, it may not work.\n",
        "# Because of this, make sure to avoid collecting multiple dataset.\n",
        "# You can attach multiple datasets to a project at a time, and should principally be able to work with multiple in code.\n",
        "# dataset_name must match the dataset uploaded to Labelbox. reviews_triplets_example.json is not, so this will not work.\n",
        "dataset_name = \"reviews_triplets_example.json\"\n",
        "dataset = next(iter(client.get_datasets(where=Dataset.name == dataset_name)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZFxKTyIH-Op"
      },
      "source": [
        "# This code sets the external_ids of any datarows within the dataset that do not have an external_id.\n",
        "# WARNING: THIS CODE CAN MODIFY DATA YOU HAVE UPLOADED TO THE CLOUD.\n",
        "# I recommend you modify this code to make it safer and unable to modify data in a negative way.\n",
        "# For example, in the GraphQL query, it might be possible to not pass row_data, which would prevent row_data from getting deleted for any reason.\n",
        "# The purpose of this is to ensure that each datarow has an external_id (which is <A_id>_<B_id>_<C_id>) so we can set the priority of these datarows\n",
        "\n",
        "for i in iter(dataset.data_rows()):\n",
        "  if i.external_id is None:\n",
        "    compare = json.loads(i.row_data.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\x13\", \" \").replace(\"\\x10\", \" \"))\n",
        "\n",
        "    row_data = i.row_data.replace('\"', \"\\\\\" + '\"').replace(\"\\\\T\", \"T\").replace(\"\\\\i\", \"i\").replace(\n",
        "                \"\\u0013\", \" \").replace(\"\\\\c\", \"c\").replace(\"\\\\M\", \"M\").replace(\"\\\\P\", \"P\").replace(\n",
        "                \"\\\\d\", \"d\").replace(\"\\\\<\", \"\").replace(\"\\\\ \", \" \").replace(\"\\\\w\", \"w\").replace(\"\\\\\\\\\", \"\\\\\").replace(\n",
        "                \"\\\\s\", \"s\").replace(\"\\\\2\", \"2\").replace(\"\\\\5\", \"5\").replace(\"\\\\S\", \"S\").replace(\"\\u0010\", \" \")\n",
        "    ex_id = str(compare[\"compare\"][\"A_id\"]) + \"_\" + str(compare[\"compare\"][\"B_id\"]) + \"_\" + str(compare[\"compare\"][\"C_id\"])\n",
        "    \n",
        "    client.execute(\n",
        "        f'''\n",
        "        mutation UpdateDataRow {{\n",
        "            updateDataRow( \n",
        "                where: {{ \n",
        "                  id: \"{i.uid}\" \n",
        "                }},\n",
        "                data: {{\n",
        "                    externalId: \"{ex_id}\",\n",
        "                    rowData: \"{row_data}\"\n",
        "                }}\n",
        "            ) {{ \n",
        "              id \n",
        "            }}\n",
        "        }}\n",
        "        '''\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE0CQyuFJFn7"
      },
      "source": [
        "# This code can add a column to a Pandas dataframe for the external_ids given a local instance of the reviews triplets json file.\n",
        "\n",
        "reviews_triplets = pd.read_json(dataPath + \"reviews_triplets_example.json\")\n",
        "reviews_triplets[\"external_ids\"] = reviews_triplets[\"data\"].apply(lambda x: str(json.loads(x.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\x13\", \" \").replace(\"\\x10\", \" \"))[\"compare\"][\"A_id\"]) + \"_\" + \n",
        "                                                          str(json.loads(x.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\x13\", \" \").replace(\"\\x10\", \" \"))[\"compare\"][\"B_id\"]) + \"_\" + \n",
        "                                                          str(json.loads(x.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\x13\", \" \").replace(\"\\x10\", \" \"))[\"compare\"][\"C_id\"]))\n",
        "\n",
        "# Then, we can add the uids, which are ids created by Labelbox and used for active learning.\n",
        "\n",
        "def update_with_uid(\n",
        "        dataframe: pd.core.frame.DataFrame,\n",
        "        dataset: schema.dataset.Dataset) -> pd.core.frame.DataFrame:\n",
        "    \"\"\"Add uid column for tracking labelbox's id of the same datarow in our dataframe.\n",
        "    Args:\n",
        "        df: dataframe to augment\n",
        "        project_name: project name for dataset\n",
        "        dataset_name: name for dataset\n",
        "    \"\"\"    \n",
        "    external_uid_map = {\n",
        "        str(data_row.external_id): data_row.uid\n",
        "        for data_row in dataset.data_rows()\n",
        "    }\n",
        "    dataframe['uid'] = dataframe.external_ids.map(external_uid_map)\n",
        "    return dataframe\n",
        "\n",
        "update_with_uid(reviews_triplets, dataset)\n",
        "\n",
        "# After that, we have two more columns for our reviews_triplets dataframe, both of which are very useful to us:\n",
        "reviews_triplets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Vm7YfIJpa8"
      },
      "source": [
        "# Now, we can begin to set the priority of different data points.\n",
        "\n",
        "# Set a random priority for a proof-of-concept.\n",
        "def set_priority():\n",
        "  return np.random.randint(1, 100)\n",
        "\n",
        "\n",
        "\n",
        "# This code was largely adapted from the Active Learning Tutorial linked above.\n",
        "# Currently, the priority is simply set to i, meaning the first datarows have the highest priority in labeling.\n",
        "# Replacing i with set_priority() under the \"priority:\" parameter would set it to a random number between 1 and 100.\n",
        "# This code appears to work, but has been difficult to test.\n",
        "\n",
        "project_id = project.uid\n",
        "uids = reviews_triplets.uid\n",
        "\n",
        "# priority_data_rows = (\n",
        "#     f'{{dataRow: {{id: \"{uid}\"}}, priority: {i}, numLabels: 1}}'\n",
        "#     for i, uid in enumerate(uids)\n",
        "# )\n",
        "\n",
        "def gen_priority_data_rows():\n",
        "  for i, uid in enumerate(uids):\n",
        "    print(uid)\n",
        "    if(i > 5):\n",
        "      break;\n",
        "    yield f'''{{dataRow: {{id: \"{uid}\"}}, priority: {i}, numLabels: 1}}'''\n",
        "priority_data_rows = gen_priority_data_rows()\n",
        "\n",
        "#data_rows = chain(priority_data_rows, rest_data_rows)\n",
        "data_rows = priority_data_rows\n",
        "\n",
        "def batches(iterable, size):\n",
        "    iterator = iter(iterable)\n",
        "    for first in iterator:\n",
        "        yield chain([first], islice(iterator, size - 1))\n",
        "\n",
        "for batch in batches(data_rows, size=999):\n",
        "    response = client.execute(\n",
        "        f'''\n",
        "        mutation setLabelingParameterOverrides {{\n",
        "          project(where: {{ id: \"{project_id}\" }}) {{\n",
        "            setLabelingParameterOverrides(data: [\n",
        "                {','.join(batch)}\n",
        "            ]) {{\n",
        "              success\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "        '''\n",
        "    )\n",
        "    assert not response.get('errors')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxMbhbN4LoqY"
      },
      "source": [
        "# To unset all of the labeling parameter overrides (mainly the labeling priority), run this:\n",
        "response = client.execute(f'''\n",
        "    mutation UnsetAllLabelingParameterOverrides {{\n",
        "\n",
        "      project(where: {{id: \"{project.uid}\" }}) {{\n",
        "        unsetAllLabelingParameterOverrides {{\n",
        "            success\n",
        "            deletedCount\n",
        "        }}\n",
        "      }}\n",
        "    }}\n",
        "    '''\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l99oyevgL3hc"
      },
      "source": [
        "# To check the labeling parameter overrides, run this:\n",
        "client.execute(f'''\n",
        "    query PullLabelingParameterOverrides {{\n",
        "\n",
        "      project(where: {{id: \"{project.uid}\" }}) {{\n",
        "        labelingParameterOverrides {{\n",
        "            id\n",
        "            priority\n",
        "        }}\n",
        "      }}\n",
        "    }}\n",
        "    '''\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNGc-_IpMB3T"
      },
      "source": [
        "TODO:\n",
        "- Test if the labeling parameter overrides are actually working in the Labelbox interface.\n",
        "- If these tests pass, it could be used in a model where the most uncertain datapoints are prompted to the researcher.\n",
        "- Currently, it's been hard for us to test this, because it appears that the uids for the labelingParameterOverrides we pull are completely different uids than the uids in the dataset we're using. This raises a few questions about how we could test this method."
      ]
    }
  ]
}